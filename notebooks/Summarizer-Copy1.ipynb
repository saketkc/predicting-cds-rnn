{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:04:08.615435Z",
     "start_time": "2017-11-27T23:04:05.627979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "orthology = pd.read_table('/home/cmb-panasas2/skchoudh/genomes/ensemble_orthology/human-mouse/mouse_query.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:05:27.857469Z",
     "start_time": "2017-11-27T23:04:08.618249Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Embedding, Bidirectional\n",
    "#Dropout, RepeatVector, TimeDistributed, AveragePooling1D, Flatten\n",
    "from collections import defaultdict, OrderedDict\n",
    "from scipy.stats import describe\n",
    "print(K.backend())\n",
    "def set_keras_backend(backend):\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        importlib.reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "\n",
    "#set_keras_backend('theano')\n",
    "__BASES__ = ['A','C','G','T']\n",
    "__BASES_MAP__ = OrderedDict(zip(__BASES__,range(4)))\n",
    "__MERGE_KEYS__ = ['UTR5', 'CDS', 'UTR3']\n",
    "__MERGE_LABELS__ = OrderedDict(zip(__MERGE_KEYS__, range(3)))\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "def _downsample(genes_histogram, genes_to_keep=10000):\n",
    "    \"\"\"Downsample a histogram by randomly dropping proportional\n",
    "    number of genes in each bin\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "\n",
    "    genes_histogram : dict\n",
    "        Dictionary with format {bin:[list of genes in bin]}\n",
    "\n",
    "    genes_to_keep : int\n",
    "        Total genes to keep\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    downsampled_dict : dict\n",
    "        Dictionary with downsampled list in each bin\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    downsampled_dict = {}\n",
    "    total_bins = len(genes_histogram)\n",
    "    total_genes = sum([len(x) for x in list(genes_histogram.values())])\n",
    "    scaling_factor = genes_to_keep / total_genes\n",
    "    for bin_index, genes_in_bin in list(genes_histogram.items()):\n",
    "        n_genes_in_bin = len(genes_in_bin)\n",
    "        n_genes_to_keep = int(np.ceil(n_genes_in_bin * scaling_factor))\n",
    "        index_genes_to_keep = np.random.choice(n_genes_in_bin, n_genes_to_keep)\n",
    "        genes_to_keep = np.array(genes_in_bin)[index_genes_to_keep]\n",
    "        downsampled_dict[bin_index] = list(genes_to_keep)\n",
    "    return downsampled_dict\n",
    "\n",
    "\n",
    "def load_data(gene_cds, gene_lengths, genes_to_keep=10000):\n",
    "    \"\"\"Load dataset\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    gene_cds : str\n",
    "        Path to json with sequence\n",
    "\n",
    "    gene_lengths : str\n",
    "        Path to json with sequence lengths\n",
    "\n",
    "    genes_to_keep : int\n",
    "        Total genes_to_keep\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    downsampled_genes_dict : dict\n",
    "        Dictionary with format {bin:[list of genes in bin]}\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    gene_seq = OrderedDict(json.load(open(gene_cds)))\n",
    "    gene_len = OrderedDict(json.load(open(gene_lengths)))\n",
    "\n",
    "    gene_total_len = OrderedDict((k, sum(list(v.values())))\n",
    "                                 for k, v in list(gene_len.items()))\n",
    "    all_lengths = np.array(list(gene_total_len.values()))\n",
    "    valid_genes_dict = OrderedDict((k, v)\n",
    "                                   for k, v in list(gene_total_len.items())\n",
    "                                   if v < 10000)\n",
    "\n",
    "    valid_genes_keys = list(valid_genes_dict.keys())\n",
    "    valid_genes_values = list(valid_genes_dict.values())\n",
    "    hist, edges = np.histogram(valid_genes_values)\n",
    "    valid_genes_bins = np.digitize(valid_genes_values, edges)\n",
    "    length_wise_binned_genes = defaultdict(list)\n",
    "    for i, b in enumerate(valid_genes_bins):\n",
    "        length_wise_binned_genes[b - 1].append(valid_genes_keys[i])\n",
    "\n",
    "    downsampled_genes_dict = _downsample(length_wise_binned_genes,\n",
    "                                         genes_to_keep=genes_to_keep)\n",
    "    return downsampled_genes_dict, gene_seq\n",
    "\n",
    "\n",
    "def split_train_test_genes(length_wise_binned_genes, train_proportion=0.7):\n",
    "    \"\"\"Split data in training and testing set\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    length_wise_binned_genes : dict\n",
    "         Dictionary with format {bin:[list of genes in bin]}\n",
    "\n",
    "    train_proportion : float\n",
    "        Training proportion\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    training_genes : list\n",
    "\n",
    "    testing_genes : list\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    training_genes = []\n",
    "    testing_genes = []\n",
    "    for bin_number, bin_genes in list(length_wise_binned_genes.items()):\n",
    "        n_genes = len(bin_genes)\n",
    "        np.random.shuffle(bin_genes)\n",
    "        training_genes += bin_genes[:int(n_genes * train_proportion)]\n",
    "        testing_genes += bin_genes[int(n_genes * train_proportion):]\n",
    "\n",
    "    return training_genes, testing_genes\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def map_base_to_int(base):\n",
    "    \"\"\"Return int given a base character\"\"\"\n",
    "    return __BASES_MAP__[base]\n",
    "\n",
    "\n",
    "def one_hot_encoding(data_dict):\n",
    "    \"\"\"One hot encode sequences\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    data_dict : dict\n",
    "        Sequence dict as loaded from gene_cds json file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    X : array\n",
    "        X*4 Input array with columns representing A,T,G,C\n",
    "\n",
    "    Y : array\n",
    "        X*3 Labels with columns representing 5'UTR, CDS, 3'UTR\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    merged_seq = []\n",
    "    merged_label = []\n",
    "    for key in __MERGE_KEYS__:\n",
    "        merged_seq += list(data_dict[key])\n",
    "        merged_label += list([__MERGE_LABELS__[key]] * len(data_dict[key]))\n",
    "    merged_seq_int = list(map(map_base_to_int, merged_seq))\n",
    "\n",
    "    X = merged_seq_int #to_categorical(merged_seq_int)\n",
    "    Y = to_categorical(merged_label)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=4, output_dim=4))\n",
    "    model.add(Bidirectional(LSTM(\n",
    "                3,\n",
    "                return_sequences=True,\n",
    "                input_shape=(None, 4),\n",
    "                dropout=0.5,\n",
    "                recurrent_dropout=0.25)))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='rmsprop',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def load_trained_model(weights_path=None):\n",
    "    model = create_model()\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "def train(X_train, Y_train, X_test, Y_test, weights_path=None, start_epoch=0):\n",
    "    model = load_trained_model(weights_path)\n",
    "    nb_epoch = 128\n",
    "    train_history = defaultdict(list)\n",
    "    test_history = defaultdict(list)\n",
    "\n",
    "    for e in range(start_epoch, nb_epoch):\n",
    "        index = 0\n",
    "        acc_train = 0\n",
    "        for x, y in zip(X_train, Y_train):\n",
    "            acc = model.train_on_batch(np.array([x]), np.array([y]))#, verbose=0)\n",
    "            train_history[e].append(acc)\n",
    "            acc_train+=acc[1]\n",
    "            if (index%10000) == 0:\n",
    "                sys.stderr.write('Epoch: {} || Index :{} || loss: {} || acc: {}\\n'.format(e, index, acc[0], acc[1]))\n",
    "                with open('train_acc_10000.log', 'a') as f:\n",
    "                    f.write('Epoch: {} || Index :{} || loss: {} || acc: {}\\n'.format(e, index, acc[0], acc[1]))\n",
    "                ##for x_test, y_test in zip(X_test, Y_test):\n",
    "                ##    prediction = model.evaluate(np.array([x_test]),np.array([y_test]), batch_size=1)\n",
    "                ##    print (prediction)\n",
    "            index += 1\n",
    "        acc_train /= len(X_train)\n",
    "        sys.stderr.write('Epoch: {} || Train acc: {}\\n'.format(e, acc_train))\n",
    "        with open('train_acc_10000.log', 'a') as f:\n",
    "            f.write('Epoch: {} || Train acc: {}\\n'.format(e, acc_train))\n",
    "        acc_test = 0\n",
    "        for x_test, y_test in zip(X_test, Y_test):\n",
    "            #acc_test = model.predict_classes(np.array([x_test]), batch_size=1)\n",
    "            acc = model.evaluate(np.array([x_test]),np.array([y_test]), batch_size=1)\n",
    "            acc_test += acc[1]\n",
    "        acc_test /= len(X_test)\n",
    "        test_history[e].append(acc_test)\n",
    "        sys.stdout.write('Epoch: {} || Test acc: {}\\n'.format(e, acc_test))\n",
    "        with open('test_acc_10000.log', 'a') as f:\n",
    "            f.write('Epoch: {} || Test acc: {}\\n'.format(e, acc_test))\n",
    "\n",
    "        model.save('lstm-dropout_025_recur_dropout_025-epoch-{}_10000.h5'.format(e))\n",
    "\n",
    "    return model, train_history\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights_file = '../notebooks_embedding_bilstm/lstm-dropout_025_recur_dropout_025-epoch-9_10000.h5'\n",
    "last_epoch = int(re.search('epoch-\\d+', weights_file).group(0).split('-')[1])\n",
    "\n",
    "gene_cds = '../data/hg38/input/genes_cds.json'\n",
    "gene_lengths = '../data/hg38/input/genes_lengths.json'\n",
    "\n",
    "genes_dict, gene_seq = load_data(gene_cds, gene_lengths, genes_to_keep=10000)\n",
    "\n",
    "training_genes, test_genes = split_train_test_genes(genes_dict, train_proportion=0.7)\n",
    "\n",
    "## Shuffle the genes once again to avoid any bin wise correlation\n",
    "\n",
    "np.random.shuffle(training_genes)\n",
    "np.random.shuffle(test_genes)\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for gene in training_genes:\n",
    "    X, Y = one_hot_encoding(gene_seq[gene])\n",
    "    X_train.append(X)\n",
    "    Y_train.append(Y)\n",
    "\n",
    "for gene in test_genes:\n",
    "    X, Y = one_hot_encoding(gene_seq[gene])\n",
    "    X_test.append(X)\n",
    "    Y_test.append(Y)\n",
    "\n",
    "\n",
    "#model, trainhist = train(X_train, Y_train, X_test, Y_test, weights_file, last_epoch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:05:29.624742Z",
     "start_time": "2017-11-27T23:05:27.859901Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_trained_model(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:24:13.242824Z",
     "start_time": "2017-11-27T23:24:13.076116Z"
    }
   },
   "outputs": [],
   "source": [
    "mouse_gene_cds = '../data/mm10/input/genes_cds.json'\n",
    "mouse_gene_lengths = '../data/mm10/input/genes_lengths.json'\n",
    "\n",
    "gene_lengths_dict = json.load(open(gene_lengths))\n",
    "mouse_gene_lengths_dict = json.load(open(mouse_gene_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:24:53.320046Z",
     "start_time": "2017-11-27T23:24:16.389393Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "mouse_genes_dict, mouse_gene_seq = load_data(mouse_gene_cds, mouse_gene_lengths, \n",
    "                                             genes_to_keep=20108)\n",
    "\n",
    "mouse_training_genes, mouse_test_genes = split_train_test_genes(mouse_genes_dict, train_proportion=0.7)\n",
    "\n",
    "mouse_X_train = []\n",
    "mouse_Y_train = []\n",
    "\n",
    "mouse_X_test = []\n",
    "mouse_Y_test = []\n",
    "\n",
    "mouse_training_genes_filtered = []\n",
    "mouse_test_genes_filtered  = []\n",
    "\n",
    "for gene in mouse_training_genes:\n",
    "    if 'N' in ('').join(mouse_gene_seq[gene].values()):\n",
    "        continue\n",
    "    X, Y = one_hot_encoding(mouse_gene_seq[gene])\n",
    "    mouse_X_train.append(X)\n",
    "    mouse_Y_train.append(Y)\n",
    "    mouse_training_genes_filtered.append(gene)\n",
    "\n",
    "\n",
    "for gene in mouse_test_genes:\n",
    "    if 'N' in ('').join(mouse_gene_seq[gene].values()):\n",
    "        continue\n",
    "    X, Y = one_hot_encoding(mouse_gene_seq[gene])\n",
    "    mouse_X_test.append(X)\n",
    "    mouse_Y_test.append(Y)\n",
    "    mouse_test_genes_filtered.append(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T00:03:08.423433Z",
     "start_time": "2017-11-27T23:49:57.985352Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_test = []\n",
    "for  x_test, y_test in zip(X_test, Y_test):\n",
    "    #acc_test = model.predict_classes(np.array([x_test]), batch_size=1)\n",
    "    #break\n",
    "    acc = model.evaluate(np.array([x_test]),np.array([y_test]), batch_size=1, verbose=0)\n",
    "    acc_test.append(acc[1])\n",
    "acc_test = np.array(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:49:54.125018Z",
     "start_time": "2017-11-27T23:49:54.017322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.81889480352401733, 0.66801619529724121]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = model.evaluate(np.array([x_test]),np.array([y_test]), batch_size=1, verbose=0)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:48:00.563215Z",
     "start_time": "2017-11-27T23:48:00.556963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,   21,   22, ..., 1511, 1513, 1515]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:48:52.168132Z",
     "start_time": "2017-11-27T23:48:52.163177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 2, 2, 2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:45:39.682250Z",
     "start_time": "2017-11-27T23:45:39.677221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9927440881729126"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:45:53.335085Z",
     "start_time": "2017-11-27T23:45:53.329296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENSG00000125508'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_genes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:46:01.326500Z",
     "start_time": "2017-11-27T23:46:01.322060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CDS': 1464, 'UTR3': 11, 'UTR5': 41}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_lengths_dict['ENSG00000125508']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:46:10.466311Z",
     "start_time": "2017-11-27T23:46:10.243623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "acc_test = model.predict_classes(np.array([X_test[5]]), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:46:14.159083Z",
     "start_time": "2017-11-27T23:46:14.145655Z"
    }
   },
   "outputs": [],
   "source": [
    "s = ''\n",
    "for x in acc_test[0]:\n",
    "    s+=str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T00:24:02.427094Z",
     "start_time": "2017-11-28T00:24:02.418941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('00000000000000000000000000000001000110011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-27T23:46:18.120497Z",
     "start_time": "2017-11-27T23:46:18.115701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000000000000000000000000000000100011001110111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121221222'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-27T21:19:17.627Z"
    }
   },
   "outputs": [],
   "source": [
    "mouse_acc_test = []\n",
    "for  x_test, y_test in zip(mouse_X_test, mouse_Y_test):\n",
    "    #acc_test = model.predict_classes(np.array([x_test]), batch_size=1)\n",
    "    #break\n",
    "    mouse_acc = model.evaluate(np.array([x_test]),np.array([y_test]), batch_size=1, verbose=0)\n",
    "    mouse_acc_test.append(mouse_acc[1])\n",
    "mouse_acc_test = np.array(mouse_acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-27T21:19:18.538Z"
    }
   },
   "outputs": [],
   "source": [
    "mouse_acc_train = []\n",
    "for  x_train, y_train in zip(mouse_X_train, mouse_Y_train):\n",
    "    #acc_train = model.predict_classes(np.array([x_train]), batch_size=1)\n",
    "    #break\n",
    "    mouse_acc = model.evaluate(np.array([x_train]),np.array([y_train]), batch_size=1, verbose=0)\n",
    "    mouse_acc_train.append(mouse_acc[1])\n",
    "mouse_acc_train = np.array(mouse_acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35_rnn]",
   "language": "python",
   "name": "conda-env-py35_rnn-py"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "Zotero.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
